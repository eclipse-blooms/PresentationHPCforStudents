{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to write Pytorch code that isn't (too) bad\n",
    "\n",
    "This is a guide to avoiding some common pitfalls, not to writing the fastest code possible (you probably wouldn't have time for that anyway).\n",
    "\n",
    "### Motivation\n",
    "\n",
    "- Compute is an expensive shared resource.\n",
    "- Wasting compute means literally just creating entropy (__CLIMATE CHANGE IS A THING__).\n",
    "- Other people also have projects they want to do.\n",
    "\n",
    "### Some online guides that go more in-depth\n",
    "\n",
    "[Official Performance Tuning Guide](https://docs.pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
    "[Official PyTorch Guide on CUDA](https://docs.pytorch.org/docs/stable/notes/cuda.html)\n",
    "\n",
    "### A Note on training on more than one GPU\n",
    "\n",
    "Long story short, you probably don't need to.\n",
    "\n",
    "#### But I want my model to be faster\n",
    "\n",
    "PyTorch supports training on more than one GPU with [Distributed Data Parallel](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html). Implementing it can be fairly straightforward, depending on usecase.\n",
    "\n",
    "_However_, training performance usually scales sublinear with GPU count, that is to say, the performance gain is likely not going to be that big, especially considering the increased usage of limited shared resources.\n",
    "\n",
    "Training on multiple GPUs introduces significant, unavoidable overhead. If you don't know what you are doing, spending effort on doing it is likely not worth it for student projects.\n",
    "\n",
    "#### But my model doesn't fit in one GPU\n",
    "\n",
    "There are models (especially modern LLMs) that require more than the 50-80 GBs of V-Ram a typical Datacenter GPU can offer. If you need to work with a model like that, multi GPU might be hard to avoid. Luckily for you, most libraries centered around these models (like [transformers](https://huggingface.co/docs/transformers/index)) will handle the hard work for you. Consult their documentation.\n",
    "\n",
    "If you are working with large LLMs via Transformers, consider using a [Quantized Model](https://huggingface.co/docs/transformers/quantization/overview) and consult their [documentation](https://huggingface.co/docs/transformers/v5.0.0rc2/en/llm_tutorial_optimization#1-lower-precision) for other ways of reducing memory requirements, such as lowering precision or paging parts of the model. Multi-GPU should be treated as a last resort.\n",
    "\n",
    "### We now switch to a bigger Model to better see the speedups\n",
    "\n",
    "(The code below is based on the aforementioned GPT2 from scratch tutorial.)"
   ],
   "id": "ccb6f1be29077214"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from src.gpt import GPT, GPTConfig\n",
    "from src.dataloader import DataLoader\n",
    "import tiktoken\n",
    "import math\n",
    "import torch\n",
    "import time\n",
    "\n",
    "total_batch_size = 524288\n",
    "B, T = 16, 1024\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "\n",
    "device = \"cuda:2\"\n",
    "\n",
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "\n",
    "    elif it > max_steps:\n",
    "        return min_lr\n",
    "\n",
    "    else:\n",
    "        decay_ratio = (it -warmup_steps) / (max_steps - warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "        return min_lr + coeff * (max_lr - min_lr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "train_loader = DataLoader(B, T, 1, 1)\n",
    "\n",
    "\n",
    "def train(it):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "avg_batch_time = 0\n",
    "for i in range(max_steps):\n",
    "    start_batch = time.time()\n",
    "\n",
    "    train(i)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_batch = time.time()\n",
    "    batch_time = int((end_batch - start_batch) * 1000)\n",
    "    print(f\"step {i + 1}, loss: {loss}, {batch_time}ms elapsed\")\n",
    "    avg_batch_time += batch_time\n",
    "end = time.time()\n",
    "print(f\"{int((end - start) * 1000)}ms elapsed, {avg_batch_time / max_steps}ms avg batch time\")"
   ],
   "id": "c7811f9ce7831fe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Enable Tensor Cores (if applicable)\n",
    "\n",
    "Your GPU might not support this (Nvidia GPUs for datacenters like KIGS or the HPC in Erlangen do). Most libraries that implements models (e.g. transformers etc.) will either do this by default or let you enable it through them.\n",
    "\n",
    "Reduce length of Mantissa in 32-bit floating point numbers from 23 to 10 (TensorFloat-32) or 7 (bfloat16) while keeping exponent length the same to preserve range of values."
   ],
   "id": "226f08f457e94ced"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.set_float32_matmul_precision('high') #TF32\n",
    "torch.set_float32_matmul_precision('medium') #BF16\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "train_loader = DataLoader(B, T, 1, 1)\n",
    "\n",
    "\n",
    "def train(it):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "avg_batch_time = 0\n",
    "for i in range(max_steps):\n",
    "    start_batch = time.time()\n",
    "\n",
    "    train(i)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_batch = time.time()\n",
    "    batch_time = int((end_batch - start_batch) * 1000)\n",
    "    print(f\"step {i + 1}, {batch_time}ms elapsed\")\n",
    "    avg_batch_time += batch_time\n",
    "end = time.time()\n",
    "print(f\"{int((end - start) * 1000)}ms elapsed, {avg_batch_time / max_steps}ms avg batch time\")"
   ],
   "id": "197370f5a79e8631",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "WHAT IS A TENSOR CORE\n",
    "\n",
    "\n",
    "### torch.compile is really neat\n",
    "\n",
    "https://docs.pytorch.org/docs/stable/compile/programming_model.html\n",
    "\n",
    "https://www.youtube.com/live/rew5CSUaIXg\n",
    "\n",
    "https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0#heading=h.ivdr7fmrbeab\n",
    "\n",
    "You might need to install Triton though. It can usually be installed like any python package. [Windows version](https://github.com/woct0rdho/triton-windows) also exists now.\n"
   ],
   "id": "39b75630986a1f3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "\n",
    "model = torch.compile(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "train_loader = DataLoader(B, T, 1, 1)\n",
    "\n",
    "\n",
    "def train(it):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "avg_batch_time = 0\n",
    "for i in range(max_steps):\n",
    "    start_batch = time.time()\n",
    "\n",
    "    train(i)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_batch = time.time()\n",
    "    batch_time = int((end_batch - start_batch) * 1000)\n",
    "    print(f\"step {i + 1}, {batch_time}ms elapsed\")\n",
    "    avg_batch_time += batch_time\n",
    "end = time.time()\n",
    "print(f\"{int((end - start) * 1000)}ms elapsed, {avg_batch_time / max_steps}ms avg batch time\")"
   ],
   "id": "4cfebd75f7c749f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "for inference you can compile you model, for training it's best practice to compile a train function instead\n",
    "\n",
    "reduce overhead arg can help with small models/batches (try it out in each usecase)\n",
    "\n",
    "avoid graph breaks; no non pytorch stuff in compile\n",
    "\n",
    "best practices: avoid inplace ops when possible, AOTAutograd sometimes has issues with them"
   ],
   "id": "62ec831d09f33693"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Multithreading is very simple and you should probably use it\n",
    "\n",
    "TODO MAKE SOMETHING WITH THIS\n",
    "\n",
    "- example: pipeline processing image timeseries:\n",
    "\t- real world usecase: sequential steps, process as fast as possible\n",
    "\t- naive training approach follows same logic: proprocessing first step on cpu, cnn on first frame, first rnn step, proprocessing second step, cnn on second frame, seocnd rnn step\n",
    "\t- batch cnn step, multithread, avoid idle gpu time\n",
    "\t- THIS  IS OBVIOUS WHEN YOU THINK ABOUT IT, but pytorch makes it easy to not do it\n",
    "\t- you can use normal multithreading for standard python code\n",
    "\n",
    "### Nice Values for Constants\n",
    "\n",
    "Make certain constants multiples of two. This sounds super esoteric, but actually works in my experience:\n",
    "- The GPU memory is divided into blocks.\n",
    "- Accessing memory is faster if it fits neatly into those blocks.\n",
    "- The memory blocks are sized in multiples of two.\n",
    "- Any constants that determine the size of tensors that will be stored in GPU memory should be multiples of two.\n",
    "\n",
    "This includes batch size, neural network layer sizes\n",
    "\n",
    "\n",
    "### Avoiding Computation of Unnecessary Gradients\n",
    "\n",
    "Gradients are only needed when you actually plan to use them for training. They should be disabled otherwise."
   ],
   "id": "9f5e7c98f55e8acb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:2\")\n",
    "\n",
    "model = Net(num_classes)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "X, y = get_data(device)"
   ],
   "id": "cae46db8d5141d30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5b9ed03a81570b8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "During Inference, instead of this",
   "id": "1517bdbfb5e1d63c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_hat = model(X)",
   "id": "427e3303e41464f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "do\n",
    "this:"
   ],
   "id": "70c72d3bbfcafaba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    y_hat = model(X)"
   ],
   "id": "289b7e7a403a64b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    ".detach!!!!!!\n",
    "requires_grad"
   ],
   "id": "49e8294bfee0b7fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e70c90928f05528c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
