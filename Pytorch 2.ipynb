{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to write Pytorch code that isn't (too) bad\n",
    "\n",
    "This is a guide to avoiding some common pitfalls, not to writing the fastest code possible (you probably wouldn't have time for that anyway).\n",
    "\n",
    "### Motivation\n",
    "\n",
    "- Compute is an expensive shared resource.\n",
    "- Wasting compute means literally just creating entropy (__CLIMATE CHANGE IS A THING__).\n",
    "- Other people also have projects they want to do.\n",
    "\n",
    "### Some online guides that go more in-depth\n",
    "\n",
    "- [Official Performance Tuning Guide](https://docs.pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
    "- [Official PyTorch Guide on CUDA](https://docs.pytorch.org/docs/stable/notes/cuda.html)\n",
    "\n",
    "### A Note on training on more than one GPU\n",
    "\n",
    "Long story short, you probably don't need to.\n",
    "\n",
    "#### But I want my model to be faster\n",
    "\n",
    "PyTorch supports training on more than one GPU with [Distributed Data Parallel](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html). Implementing it can be fairly straightforward, depending on usecase.\n",
    "\n",
    "_However_, training performance usually scales sublinear with GPU count, that is to say, the performance gain is likely not going to be that big, especially considering the increased usage of limited shared resources.\n",
    "\n",
    "Training on multiple GPUs introduces significant, unavoidable overhead. If you don't know what you are doing, spending effort on doing it is likely not worth it for student projects.\n",
    "\n",
    "#### But my model doesn't fit in one GPU\n",
    "\n",
    "There are models (especially modern LLMs) that require more than the 50-80 GBs of V-Ram a typical Datacenter GPU can offer. If you need to work with a model like that, multi GPU might be hard to avoid. Luckily for you, most libraries centered around these models (like [transformers](https://huggingface.co/docs/transformers/index)) will handle the hard work for you. Consult their documentation.\n",
    "\n",
    "If you are working with large LLMs via Transformers, consider using a [Quantized Model](https://huggingface.co/docs/transformers/quantization/overview) and consult their [documentation](https://huggingface.co/docs/transformers/v5.0.0rc2/en/llm_tutorial_optimization#1-lower-precision) for other ways of reducing memory requirements, such as lowering precision or paging parts of the model. Multi-GPU should be treated as a last resort.\n",
    "\n",
    "### We now switch to a bigger Model to better see the speedups\n",
    "\n",
    "(The code below is based on the aforementioned GPT2 from scratch tutorial.)"
   ],
   "id": "ccb6f1be29077214"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-21T11:58:10.055076526Z",
     "start_time": "2026-01-21T11:58:10.044664470Z"
    }
   },
   "source": [
    "from src.training import *\n",
    "device = \"cuda:2\""
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:58:43.348785589Z",
     "start_time": "2026-01-21T11:58:10.055827161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "train_loader = DataLoader(B, T, 1, 1)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "avg_batch_time = 0\n",
    "for i in range(max_steps):\n",
    "    start_batch = time.time()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_batch = time.time()\n",
    "    batch_time = int((end_batch - start_batch) * 1000)\n",
    "    print(f\"step {i + 1}, {batch_time}ms elapsed\")\n",
    "    avg_batch_time += batch_time\n",
    "end = time.time()\n",
    "print(f\"{int((end - start) * 1000)}ms elapsed, {avg_batch_time / max_steps}ms avg batch time\")"
   ],
   "id": "c7811f9ce7831fe2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338020 tokens\n",
      "1 epoch = 20 batches\n",
      "step 1, 803ms elapsed\n",
      "step 2, 554ms elapsed\n",
      "step 3, 588ms elapsed\n",
      "step 4, 572ms elapsed\n",
      "step 5, 574ms elapsed\n",
      "step 6, 580ms elapsed\n",
      "step 7, 574ms elapsed\n",
      "step 8, 581ms elapsed\n",
      "step 9, 580ms elapsed\n",
      "step 10, 575ms elapsed\n",
      "step 11, 587ms elapsed\n",
      "step 12, 577ms elapsed\n",
      "step 13, 582ms elapsed\n",
      "step 14, 581ms elapsed\n",
      "step 15, 581ms elapsed\n",
      "step 16, 577ms elapsed\n",
      "step 17, 577ms elapsed\n",
      "step 18, 583ms elapsed\n",
      "step 19, 588ms elapsed\n",
      "step 20, 583ms elapsed\n",
      "step 21, 586ms elapsed\n",
      "step 22, 586ms elapsed\n",
      "step 23, 591ms elapsed\n",
      "step 24, 578ms elapsed\n",
      "step 25, 603ms elapsed\n",
      "step 26, 584ms elapsed\n",
      "step 27, 602ms elapsed\n",
      "step 28, 588ms elapsed\n",
      "step 29, 604ms elapsed\n",
      "step 30, 588ms elapsed\n",
      "step 31, 595ms elapsed\n",
      "step 32, 594ms elapsed\n",
      "step 33, 599ms elapsed\n",
      "step 34, 590ms elapsed\n",
      "step 35, 604ms elapsed\n",
      "step 36, 592ms elapsed\n",
      "step 37, 597ms elapsed\n",
      "step 38, 599ms elapsed\n",
      "step 39, 598ms elapsed\n",
      "step 40, 601ms elapsed\n",
      "step 41, 601ms elapsed\n",
      "step 42, 603ms elapsed\n",
      "step 43, 592ms elapsed\n",
      "step 44, 604ms elapsed\n",
      "step 45, 600ms elapsed\n",
      "step 46, 602ms elapsed\n",
      "step 47, 606ms elapsed\n",
      "step 48, 601ms elapsed\n",
      "step 49, 604ms elapsed\n",
      "step 50, 605ms elapsed\n",
      "29748ms elapsed, 593.88ms avg batch time\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Reducing precision for performance gains\n",
    "\n",
    "#### Matrix multiplication\n",
    "\n",
    "`torch.set_float32_matmul_precision` reduces precision during matrix multiplication on CUDA devices to accelerate computation with minimal numerical effects ([Details](https://docs.pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html)). This allows the GPU to uses dedicated \"Tensor Cores\" which are optimized 16bit matrix multiplication.\n",
    "\n",
    "Your GPU might not support this (Nvidia GPUs for datacenters like KIGS or the HPC in Erlangen do). Most libraries that implements models (e.g. transformers etc.) will either do this by default or let you enable it through them.\n",
    "\n",
    "#### Autocast\n",
    "\n",
    "Using `with torch.autocast` automatically reduces the length of Mantissa in 32-bit floating point numbers from 23 to 10 (TensorFloat-32) or 7 (bfloat16) in supported operations while keeping exponent length the same to preserve range of values. This improves performance and memory usage at usually negligible costs to precision. It's best practice to only wrap the forward pass in the autocast, as the loss of precision can lead to issues in other places in some cases."
   ],
   "id": "226f08f457e94ced"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:58:53.407612904Z",
     "start_time": "2026-01-21T11:58:43.382117658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#reduce precision for matrix multiplication\n",
    "torch.set_float32_matmul_precision('high') #TF32\n",
    "torch.set_float32_matmul_precision('medium') #BF16\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "train_loader = DataLoader(B, T, 1, 1)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "avg_batch_time = 0\n",
    "for i in range(max_steps):\n",
    "    start_batch = time.time()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16): #bf16 for forward pass only\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_batch = time.time()\n",
    "    batch_time = int((end_batch - start_batch) * 1000)\n",
    "    print(f\"step {i + 1}, {batch_time}ms elapsed\")\n",
    "    avg_batch_time += batch_time\n",
    "end = time.time()\n",
    "print(f\"{int((end - start) * 1000)}ms elapsed, {avg_batch_time / max_steps}ms avg batch time\")"
   ],
   "id": "197370f5a79e8631",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338020 tokens\n",
      "1 epoch = 20 batches\n",
      "step 1, 194ms elapsed\n",
      "step 2, 152ms elapsed\n",
      "step 3, 148ms elapsed\n",
      "step 4, 151ms elapsed\n",
      "step 5, 148ms elapsed\n",
      "step 6, 152ms elapsed\n",
      "step 7, 150ms elapsed\n",
      "step 8, 151ms elapsed\n",
      "step 9, 150ms elapsed\n",
      "step 10, 150ms elapsed\n",
      "step 11, 150ms elapsed\n",
      "step 12, 149ms elapsed\n",
      "step 13, 143ms elapsed\n",
      "step 14, 153ms elapsed\n",
      "step 15, 149ms elapsed\n",
      "step 16, 151ms elapsed\n",
      "step 17, 150ms elapsed\n",
      "step 18, 150ms elapsed\n",
      "step 19, 150ms elapsed\n",
      "step 20, 150ms elapsed\n",
      "step 21, 154ms elapsed\n",
      "step 22, 149ms elapsed\n",
      "step 23, 151ms elapsed\n",
      "step 24, 150ms elapsed\n",
      "step 25, 151ms elapsed\n",
      "step 26, 150ms elapsed\n",
      "step 27, 151ms elapsed\n",
      "step 28, 151ms elapsed\n",
      "step 29, 148ms elapsed\n",
      "step 30, 152ms elapsed\n",
      "step 31, 148ms elapsed\n",
      "step 32, 152ms elapsed\n",
      "step 33, 150ms elapsed\n",
      "step 34, 150ms elapsed\n",
      "step 35, 151ms elapsed\n",
      "step 36, 152ms elapsed\n",
      "step 37, 151ms elapsed\n",
      "step 38, 149ms elapsed\n",
      "step 39, 151ms elapsed\n",
      "step 40, 149ms elapsed\n",
      "step 41, 151ms elapsed\n",
      "step 42, 152ms elapsed\n",
      "step 43, 152ms elapsed\n",
      "step 44, 151ms elapsed\n",
      "step 45, 150ms elapsed\n",
      "step 46, 153ms elapsed\n",
      "step 47, 151ms elapsed\n",
      "step 48, 151ms elapsed\n",
      "step 49, 152ms elapsed\n",
      "step 50, 151ms elapsed\n",
      "7598ms elapsed, 151.3ms avg batch time\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Looking under the hood\n",
    "\n",
    "### Resources\n",
    "- [Official PyTorch Docs for compiler](https://docs.pytorch.org/docs/stable/torch.compiler.html)\n",
    "- [Official Tutorial for working with compile](https://docs.pytorch.org/docs/stable/compile/programming_model.html)\n",
    "- [A guide on integrating control flow into high performance PyTorch](https://blog.ezyang.com/2025/09/so-you-want-to-control-flow-in-pt2/)\n",
    "- [torch.compile, the missing manual](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0#heading=h.ivdr7fmrbeab) - this is probably the best resource for working with compile out there ([There is also an overview video for it](https://www.youtube.com/live/rew5CSUaIXg))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Autograd\n",
    "\n",
    "Autograd is the engine that pytorch uses to perform automatic differentiation. When a operation is performed on a tensor, the tensor stores that information, building a graph like structure that contains a \"history\" of what happened to that tensor. According to the chain rule, one can now differentiate all those operations in reverse order to arrive at the derivative of the tensor. Autograd does this when `.backward()` is called.\n",
    "\n",
    "### Dynamo\n",
    "\n",
    "Dynamo is an at runtime just in time compiler that turns your Python code into a computation graph. This computation graph is then used by a Compiler like Triton to produce bytecode which can run much faster than native Python.\n",
    "\n",
    "Given the more static nature of the produced bytecode, Dynamo uses so-called _Guards_ to perform periodic checks on the processed tensors. This prevents errors due to shape mismatches etc. by forcing a recompilations if inputs change. These recompilations slow performance and should generally be avoided.\n",
    "\n",
    "### Inductor\n",
    "\n",
    "Inductor serves as interface between low level backends like Triton and Dynamo. It also optimizes the graphs produced by Dynamo to produce better Kernels\n",
    "\n",
    "### The CPU-GPU bottleneck\n",
    "\n",
    "PyTorch can, without any special code, run processes asynchronously on the CPU and GPU.\n",
    "\n",
    "The central element to doing so efficiently is understanding how PyTorch handles tensors on the GPU:\n",
    "\n",
    "Operations on the GPU aren't actually completed when their line of Python code is executed, merely scheduled. This means that any subsequent line of Python code can continue to run, __so long as it doesn't depend on the result of the GPU operation__.\n",
    "\n",
    "This means that there are a set of operations that force the GPU and CPU to synchronize, which can lead to (sometimes massive) idle time. These operations include:\n",
    "\n",
    "- Moving tensors, models, etc. between CPU and GPU (with `.to(device)`, `.cpu()`, `.value()`, explicit casting i.e. `.numpy()` or `int(...)`, etc.)\n",
    "- Any Python control structure (if, while, for, ...) that uses an operation on a GPU tensor to check for a truth value or iterates on one.\n",
    "- Explicitly calling `torch.cuda.synchronize()` (this has some actual use cases though, like accurately timing GPU execution time in the examples I have used)\n",
    "\n",
    "### Multithreading is very simple and you should probably use it\n",
    "\n",
    "Many model pipelines contain various preprocessing, monitoring, evaluation and logging steps that need to be performed by the cpu. As explained in the previous section, certain operations force synchronization between the cpu and gpu. The multithreading in regular python provides a very simple way for GPU and CPU processes to run simultaneously without inhibiting each other. A general guideline here is to start threads as early as possible and rejoin them as late as possible\n",
    "\n",
    "#### Very basic Strategy:\n",
    "- Time all the GPU and CPU operations separately by using `torch.cuda.synchronize()` to force python to wait for GPU operations to finish.\n",
    "- Figure out what CPU operations take significant time and try to either\n",
    "    - A: Run them in parallel to GPU operations by avoiding any CPU-GPU synchronization\n",
    "    - B: explicitely multithreading them"
   ],
   "id": "75e49f03ea479d29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:59:28.247572068Z",
     "start_time": "2026-01-21T11:58:53.444401037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#these are dummy functions used for illustration purposes\n",
    "import time\n",
    "\n",
    "def preprocessing(iteration): #example of process that needs to run before the training step\n",
    "    time.sleep(.1)\n",
    "\n",
    "def evaluate(result): #example of process that needs to run after the training step and requires the result\n",
    "    time.sleep(.4)\n",
    "\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "train_loader = DataLoader(B, T, 1, 1)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "avg_batch_time = 0\n",
    "for i in range(max_steps):\n",
    "    start_batch = time.time()\n",
    "\n",
    "    preprocessing(i)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16): #bf16 for forward pass only\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    evaluate(loss.cpu().detach())\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_batch = time.time()\n",
    "    batch_time = int((end_batch - start_batch) * 1000)\n",
    "    print(f\"step {i + 1}, {batch_time}ms elapsed\")\n",
    "    avg_batch_time += batch_time\n",
    "end = time.time()\n",
    "print(f\"{int((end - start) * 1000)}ms elapsed, {avg_batch_time / max_steps}ms avg batch time\")"
   ],
   "id": "7bad48c51260b4bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338020 tokens\n",
      "1 epoch = 20 batches\n",
      "step 1, 657ms elapsed\n",
      "step 2, 650ms elapsed\n",
      "step 3, 647ms elapsed\n",
      "step 4, 652ms elapsed\n",
      "step 5, 647ms elapsed\n",
      "step 6, 651ms elapsed\n",
      "step 7, 644ms elapsed\n",
      "step 8, 659ms elapsed\n",
      "step 9, 646ms elapsed\n",
      "step 10, 649ms elapsed\n",
      "step 11, 650ms elapsed\n",
      "step 12, 648ms elapsed\n",
      "step 13, 646ms elapsed\n",
      "step 14, 647ms elapsed\n",
      "step 15, 654ms elapsed\n",
      "step 16, 648ms elapsed\n",
      "step 17, 645ms elapsed\n",
      "step 18, 647ms elapsed\n",
      "step 19, 646ms elapsed\n",
      "step 20, 649ms elapsed\n",
      "step 21, 652ms elapsed\n",
      "step 22, 646ms elapsed\n",
      "step 23, 651ms elapsed\n",
      "step 24, 647ms elapsed\n",
      "step 25, 650ms elapsed\n",
      "step 26, 648ms elapsed\n",
      "step 27, 648ms elapsed\n",
      "step 28, 650ms elapsed\n",
      "step 29, 649ms elapsed\n",
      "step 30, 647ms elapsed\n",
      "step 31, 654ms elapsed\n",
      "step 32, 648ms elapsed\n",
      "step 33, 648ms elapsed\n",
      "step 34, 649ms elapsed\n",
      "step 35, 648ms elapsed\n",
      "step 36, 645ms elapsed\n",
      "step 37, 652ms elapsed\n",
      "step 38, 645ms elapsed\n",
      "step 39, 652ms elapsed\n",
      "step 40, 644ms elapsed\n",
      "step 41, 649ms elapsed\n",
      "step 42, 649ms elapsed\n",
      "step 43, 652ms elapsed\n",
      "step 44, 647ms elapsed\n",
      "step 45, 648ms elapsed\n",
      "step 46, 646ms elapsed\n",
      "step 47, 648ms elapsed\n",
      "step 48, 651ms elapsed\n",
      "step 49, 647ms elapsed\n",
      "step 50, 647ms elapsed\n",
      "32478ms elapsed, 648.78ms avg batch time\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:59:38.763453823Z",
     "start_time": "2026-01-21T11:59:28.280169943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import threading\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "train_loader = DataLoader(B, T, 1, 1)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "avg_batch_time = 0\n",
    "preprocessing_thread = threading.Thread(target=preprocessing, args=(0,))\n",
    "preprocessing_thread.start() # start immediately for first sample\n",
    "\n",
    "eval_threads = [] #collect evaluation threads\n",
    "\n",
    "for i in range(max_steps):\n",
    "    start_batch = time.time()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    preprocessing_thread.join() # needs to finish before step is performed\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16): #bf16 for forward pass only\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    #restart preprocessing immediately for next iteration\n",
    "    preprocessing_thread = threading.Thread(target=preprocessing, args=(i+1,))\n",
    "    preprocessing_thread.start()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #start evaluation as soon as possible\n",
    "    evaluation_thread = threading.Thread(target=evaluate, args=(loss.cpu().detach(),))\n",
    "    evaluation_thread.start()\n",
    "\n",
    "    #collect evaluation thread so they can be stopped later\n",
    "    eval_threads.append(evaluation_thread)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_batch = time.time()\n",
    "    batch_time = int((end_batch - start_batch) * 1000)\n",
    "    print(f\"step {i + 1}, {batch_time}ms elapsed\")\n",
    "    avg_batch_time += batch_time\n",
    "\n",
    "for t in eval_threads:\n",
    "    t.join()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"{int((end - start) * 1000)}ms elapsed, {avg_batch_time / max_steps}ms avg batch time\")"
   ],
   "id": "25c6b6d8777fdaab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338020 tokens\n",
      "1 epoch = 20 batches\n",
      "step 1, 255ms elapsed\n",
      "step 2, 148ms elapsed\n",
      "step 3, 151ms elapsed\n",
      "step 4, 153ms elapsed\n",
      "step 5, 153ms elapsed\n",
      "step 6, 155ms elapsed\n",
      "step 7, 150ms elapsed\n",
      "step 8, 150ms elapsed\n",
      "step 9, 153ms elapsed\n",
      "step 10, 149ms elapsed\n",
      "step 11, 151ms elapsed\n",
      "step 12, 149ms elapsed\n",
      "step 13, 151ms elapsed\n",
      "step 14, 155ms elapsed\n",
      "step 15, 147ms elapsed\n",
      "step 16, 151ms elapsed\n",
      "step 17, 153ms elapsed\n",
      "step 18, 158ms elapsed\n",
      "step 19, 151ms elapsed\n",
      "step 20, 154ms elapsed\n",
      "step 21, 155ms elapsed\n",
      "step 22, 153ms elapsed\n",
      "step 23, 151ms elapsed\n",
      "step 24, 155ms elapsed\n",
      "step 25, 151ms elapsed\n",
      "step 26, 154ms elapsed\n",
      "step 27, 149ms elapsed\n",
      "step 28, 149ms elapsed\n",
      "step 29, 153ms elapsed\n",
      "step 30, 151ms elapsed\n",
      "step 31, 151ms elapsed\n",
      "step 32, 152ms elapsed\n",
      "step 33, 156ms elapsed\n",
      "step 34, 152ms elapsed\n",
      "step 35, 155ms elapsed\n",
      "step 36, 152ms elapsed\n",
      "step 37, 152ms elapsed\n",
      "step 38, 150ms elapsed\n",
      "step 39, 153ms elapsed\n",
      "step 40, 157ms elapsed\n",
      "step 41, 151ms elapsed\n",
      "step 42, 153ms elapsed\n",
      "step 43, 154ms elapsed\n",
      "step 44, 152ms elapsed\n",
      "step 45, 153ms elapsed\n",
      "step 46, 157ms elapsed\n",
      "step 47, 153ms elapsed\n",
      "step 48, 152ms elapsed\n",
      "step 49, 154ms elapsed\n",
      "step 50, 157ms elapsed\n",
      "8153ms elapsed, 154.48ms avg batch time\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### torch.compile is really neat\n",
    "\n",
    "torch.compile is what calls Dynamo to optimize your code. It can be used in various ways, usually either on a `nn.Module` (inference) or a function (train loop). How, when and where it is used has a big impact on results, as do certain arguments. I do not understand it well enough to give general advice.\n",
    "\n",
    "\n",
    "#### Setup\n",
    "You will need the triton backend to use compile on Nvidia GPUs. It can usually be installed like any python package. A [Windows version](https://github.com/woct0rdho/triton-windows) also exists now. There are other backends for other usecases/hardware.\n"
   ],
   "id": "39b75630986a1f3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:59:50.944607294Z",
     "start_time": "2026-01-21T11:59:38.812608640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "\n",
    "model = torch.compile(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "train_loader = DataLoader(B, T, 1, 1)\n",
    "\n",
    "\n",
    "def train(it):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "avg_batch_time = 0\n",
    "for i in range(max_steps):\n",
    "    start_batch = time.time()\n",
    "\n",
    "    train(i)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_batch = time.time()\n",
    "    batch_time = int((end_batch - start_batch) * 1000)\n",
    "    print(f\"step {i + 1}, {batch_time}ms elapsed\")\n",
    "    avg_batch_time += batch_time\n",
    "end = time.time()\n",
    "print(f\"{int((end - start) * 1000)}ms elapsed, {avg_batch_time / max_steps}ms avg batch time\")"
   ],
   "id": "4cfebd75f7c749f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338020 tokens\n",
      "1 epoch = 20 batches\n",
      "step 1, 2150ms elapsed\n",
      "step 2, 124ms elapsed\n",
      "step 3, 130ms elapsed\n",
      "step 4, 131ms elapsed\n",
      "step 5, 131ms elapsed\n",
      "step 6, 130ms elapsed\n",
      "step 7, 129ms elapsed\n",
      "step 8, 130ms elapsed\n",
      "step 9, 130ms elapsed\n",
      "step 10, 127ms elapsed\n",
      "step 11, 130ms elapsed\n",
      "step 12, 129ms elapsed\n",
      "step 13, 131ms elapsed\n",
      "step 14, 130ms elapsed\n",
      "step 15, 129ms elapsed\n",
      "step 16, 126ms elapsed\n",
      "step 17, 129ms elapsed\n",
      "step 18, 133ms elapsed\n",
      "step 19, 135ms elapsed\n",
      "step 20, 132ms elapsed\n",
      "step 21, 132ms elapsed\n",
      "step 22, 131ms elapsed\n",
      "step 23, 130ms elapsed\n",
      "step 24, 128ms elapsed\n",
      "step 25, 134ms elapsed\n",
      "step 26, 134ms elapsed\n",
      "step 27, 132ms elapsed\n",
      "step 28, 137ms elapsed\n",
      "step 29, 135ms elapsed\n",
      "step 30, 131ms elapsed\n",
      "step 31, 131ms elapsed\n",
      "step 32, 132ms elapsed\n",
      "step 33, 134ms elapsed\n",
      "step 34, 126ms elapsed\n",
      "step 35, 134ms elapsed\n",
      "step 36, 132ms elapsed\n",
      "step 37, 135ms elapsed\n",
      "step 38, 130ms elapsed\n",
      "step 39, 135ms elapsed\n",
      "step 40, 135ms elapsed\n",
      "step 41, 133ms elapsed\n",
      "step 42, 132ms elapsed\n",
      "step 43, 134ms elapsed\n",
      "step 44, 130ms elapsed\n",
      "step 45, 131ms elapsed\n",
      "step 46, 135ms elapsed\n",
      "step 47, 131ms elapsed\n",
      "step 48, 134ms elapsed\n",
      "step 49, 134ms elapsed\n",
      "step 50, 133ms elapsed\n",
      "8620ms elapsed, 171.82ms avg batch time\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Some notes and guidelines\n",
    "- The `reduce-overhead` argument can help with compiling small models/batches (try it out in each usecase)\n",
    "- Graph breaks and recompilations kill performance. Make your performance critical code as static as possible (no dynamic shape changes, no control flow) and avoid putting python operations into it.\n",
    "- Avoid inplace operations when possible (use `x = x+1` instead of `x.add(1)` etc.), AOTAutograd sometimes has issues with them and they can cause weird issues.\n",
    "\n",
    "#### Debugging\n",
    "Run something like\n",
    "```\n",
    "TORCH_LOGS=+all python gpt_demo.py\n",
    "```\n",
    "to produce logs for Dynamo, Autograd and Inductor. This will probably be overwhelming and not very helpful. Check out [this page](https://docs.pytorch.org/docs/stable/compile/programming_model.observability.html#torch-logs) for more info on more finegrained arguments. Passing `recompiles` instead of `all` can often be a good first step.\n",
    "\n",
    "TORCH_TRACE is also a thing, run it if it works (I have had issues with it before, not sure why).\n"
   ],
   "id": "62ec831d09f33693"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### Avoiding Computation of Unnecessary Gradients\n",
    "\n",
    "Autograd computes gradients by default on any tensor that has. They should be disabled otherwise."
   ],
   "id": "9f5e7c98f55e8acb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:59:51.096288134Z",
     "start_time": "2026-01-21T11:59:51.071677439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "x.requires_grad"
   ],
   "id": "451d73b60c9036dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:59:51.120580651Z",
     "start_time": "2026-01-21T11:59:51.097292142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "layer = nn.Linear(100, 100).to(\"cuda:2\")\n",
    "layer.weight.requires_grad"
   ],
   "id": "94dedd0cb1700726",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:59:51.129841656Z",
     "start_time": "2026-01-21T11:59:51.121731543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.rand(100, device=\"cuda:2\")\n",
    "y_hat = layer(x)\n",
    "print(y_hat.grad_fn)"
   ],
   "id": "243174ba5e77324a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ViewBackward0 object at 0x77f231013e20>\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`detach()` deletes the accumulated gradient.",
   "id": "1de411ec61e2bf45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:59:51.136112234Z",
     "start_time": "2026-01-21T11:59:51.130830405Z"
    }
   },
   "cell_type": "code",
   "source": "print(y_hat.detach().grad_fn)",
   "id": "6e1ea70a4accc7a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`with torch.no_grad()` decorator disables gradient computation in it's context. This is useful for forward passes during inference.",
   "id": "f97f3136aef6296a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:59:51.141851832Z",
     "start_time": "2026-01-21T11:59:51.136752026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.rand(100, device=\"cuda:2\")\n",
    "    y_hat = layer(x)\n",
    "    print(y_hat.grad_fn)"
   ],
   "id": "289b7e7a403a64b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
