{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pytorch Crash Course",
   "id": "d4b1c432bf83343c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Quick Tips for installing Pytorch\n",
    "You need the CUDA Toolkit and an Nvidia GPU.\n",
    "\n",
    "Type\n",
    "```\n",
    "nvcc --version\n",
    "```\n",
    "into your console to figure out what CUDA version you have.\n",
    "\n",
    "If you don't get a return on your own computer, you might need to install the [CUDA Toolkit](https://developer.nvidia.com/cuda/toolkit).\n",
    "###### _If you are on a remote computer, you probably can't do that on your own. Talk to an admin instead._\n",
    "\n",
    "If everything is alright the return should look roughly like this:\n",
    "```\n",
    "nvcc: NVIDIA (R) Cuda compiler driver\n",
    "Copyright (c) 2005-2024 NVIDIA Corporation\n",
    "Built on Wed_Aug_14_10:26:51_Pacific_Daylight_Time_2024\n",
    "Cuda compilation tools, release 12.6, V12.6.68\n",
    "Build cuda_12.6.r12.6/compiler.34714021_0\n",
    "```\n",
    "(I have CUDA 12.6 on the machine I ran the command)\n",
    "\n",
    "Now you can go to [pytorch's official website](pytroch.org) to download pytorch for the correct CUDA version.\n",
    "\n",
    "Pytorch seems to be okay at dealing with CUDA version mismatches, I have seen newer versions of pytorch run fine with older CUDA versions. __DON'T QUOTE ME ON THIS THOUGH__\n",
    "\n",
    "Pytorch also offers [old versions](https://pytorch.org/get-started/previous-versions/) for download, which work with older CUDA versions.\n",
    "\n"
   ],
   "id": "9159a1dd2c6dca9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why Pytorch?\n",
    "\n",
    "- Differentiation by hand is hard, PyTorch does it for you with one line of code\n",
    "- PyTorch implements a lot of other functionality as well (data handling, optimizers, model handling, loss funcitons, ...)\n",
    "- PyTorch runs fast"
   ],
   "id": "8435c3786c6b6ae7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why use GPUs for Machine Learning?\n",
    "\n",
    "#### A Demo:\n",
    "Let's fit a CNN to CIFAR10.\n",
    "(_This code is shamelessly copied from an exercise in Prof. Jain's FMI course from last year_)"
   ],
   "id": "4cdc649c457ebf50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:47:58.632957460Z",
     "start_time": "2026-01-21T11:47:54.774061626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.small_CNN import *\n",
    "\n",
    "def train(model, optimizer, device):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for batch_idx, (X, y) in enumerate(train_loader):\n",
    "            start_batch = time.time()\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            z = model(X)\n",
    "            loss = F.cross_entropy(z, y)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            end_batch = time.time()\n",
    "            batch_time = int((end_batch - start_batch) * 1000)\n",
    "        print(f\"epoch {epoch}, {batch_time}ms elapsed\")"
   ],
   "id": "78bac3aa4efb9261",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:10.063001105Z",
     "start_time": "2026-01-21T11:47:58.636206090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = Net(num_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "train(model, optimizer, device)"
   ],
   "id": "e9ac4e5267f1e9b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, 359ms elapsed\n",
      "epoch 2, 447ms elapsed\n",
      "epoch 3, 474ms elapsed\n",
      "epoch 4, 470ms elapsed\n",
      "epoch 5, 459ms elapsed\n",
      "epoch 6, 445ms elapsed\n",
      "epoch 7, 411ms elapsed\n",
      "epoch 8, 436ms elapsed\n",
      "epoch 9, 476ms elapsed\n",
      "epoch 10, 546ms elapsed\n",
      "epoch 11, 359ms elapsed\n",
      "epoch 12, 449ms elapsed\n",
      "epoch 13, 383ms elapsed\n",
      "epoch 14, 405ms elapsed\n",
      "epoch 15, 350ms elapsed\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:32.955813751Z",
     "start_time": "2026-01-21T11:54:10.079977884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:2\")\n",
    "\n",
    "model = Net(num_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "train(model, optimizer, device)"
   ],
   "id": "d1cb456a09bac34c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, 9ms elapsed\n",
      "epoch 2, 3ms elapsed\n",
      "epoch 3, 4ms elapsed\n",
      "epoch 4, 5ms elapsed\n",
      "epoch 5, 4ms elapsed\n",
      "epoch 6, 3ms elapsed\n",
      "epoch 7, 4ms elapsed\n",
      "epoch 8, 11ms elapsed\n",
      "epoch 9, 7ms elapsed\n",
      "epoch 10, 5ms elapsed\n",
      "epoch 11, 6ms elapsed\n",
      "epoch 12, 5ms elapsed\n",
      "epoch 13, 4ms elapsed\n",
      "epoch 14, 4ms elapsed\n",
      "epoch 15, 4ms elapsed\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, running on the GPU can be a lot faster. But why is that the case?\n",
    "\n",
    "Long story short: Forward- and backward-passes (Inference and Training) require massive amounts of vector/matrix (tensor) operations, which are highly parallelizable. GPUs (Graphics Processing Units) are built to solve these kinds of highly parallel operations to render pretty computer graphics, so they are also good at neural networks by accident. Datacenter GPUs nowadays are often purpose built for ML tasks, so they are even better.\n",
    "\n",
    "If you do projects on your home computer and have a decent gaming GPU, then you can probably use it for ML.\n",
    "\n",
    "If you need big computing power, the KI-GPU Server at OTH Regensburg or the NHR at FAU Erlangen can give you access to big datacenter GPUs."
   ],
   "id": "80f2302998de3ebc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Okay but how do I PyTorch?\n",
    "\n",
    "#### Online Resources\n",
    "- [Official Documentation](https://docs.pytorch.org/docs/stable/index.html): For looking up syntax etc.\n",
    "- [Official Tutorials](https://docs.pytorch.org/tutorials/index.html): This is the best place to start; also teaches a lot of general ML things.\n",
    "- [This Blog Post by Edward Z. Yang (A PyTorch Dev)](https://blog.ezyang.com/2019/05/pytorch-internals/) for a good intro to how PyTorch works internally\n",
    "- [Official PyTorch YT Channel](https://www.youtube.com/pytorch): Tons of videos on everything PyTorch related.\n",
    "- [Huggingface Tutorials](https://huggingface.co/learn): If you are specifically interested in NLP, then this will teach you just enough PyTorch to get by (while also teaching you a lot of other things).\n",
    "- [\"Let's build GPT: from scratch, in code, spelled out.\" by Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY): Not just for people interested in transformers! An excellent tutorial on working with PyTorch to build a complex model. If you are interested also look at [the follow up video on reproducing GPT-2](https://www.youtube.com/watch?v=l8pRSuU81PU)\n",
    "\n",
    "\n",
    "### You already know a lot of the syntax if you know Numpy...\n",
    "\n",
    "- PyTorch tensors represent n-dimensional arrays of various datatypes (just like ndarrays in numpy).\n",
    "- Doing linear Algebra with PyTorch is very similar to doing it with numpy.\n",
    "- A lot of numpy functions are named identically in PyTorch. (so just trying out what you would do in numpy often works)\n",
    "- Most of the ones that aren't identical have an equivalent you can find with a google search.\n",
    "\n"
   ],
   "id": "5700a9e270521bdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:32.998333170Z",
     "start_time": "2026-01-21T11:54:32.973432142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_tensor = torch.tensor([[1, 2, 3],[4, 5, 6]])\n",
    "my_tensor"
   ],
   "id": "145a7132926f14cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:33.005840051Z",
     "start_time": "2026-01-21T11:54:32.999535733Z"
    }
   },
   "cell_type": "code",
   "source": "my_tensor[1][1:]",
   "id": "875836e7d5baad89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:33.011525878Z",
     "start_time": "2026-01-21T11:54:33.006444323Z"
    }
   },
   "cell_type": "code",
   "source": "my_tensor.reshape((3,2))",
   "id": "1e2e3f041ffba89d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:33.016499002Z",
     "start_time": "2026-01-21T11:54:33.012106419Z"
    }
   },
   "cell_type": "code",
   "source": "my_tensor * 2",
   "id": "e4915927e1a95872",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6],\n",
       "        [ 8, 10, 12]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:33.027222663Z",
     "start_time": "2026-01-21T11:54:33.017092093Z"
    }
   },
   "cell_type": "code",
   "source": "my_tensor.T",
   "id": "f9f8c9221997b375",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:33.044066550Z",
     "start_time": "2026-01-21T11:54:33.028719531Z"
    }
   },
   "cell_type": "code",
   "source": "my_tensor.unsqueeze(0)",
   "id": "1b599463ea886271",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [4, 5, 6]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:33.061794173Z",
     "start_time": "2026-01-21T11:54:33.046097398Z"
    }
   },
   "cell_type": "code",
   "source": "my_tensor @ my_tensor.T #matrix multiplication",
   "id": "cc6eec695a3afd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 32],\n",
       "        [32, 77]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### The important new bit: The GPU\n",
    "Tensors need to explicitly be moved between devices."
   ],
   "id": "c2cc964aa7143204"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:33.072275500Z",
     "start_time": "2026-01-21T11:54:33.063798400Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available() #check if GPU is available",
   "id": "6e2ff99e29381227",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:33.078765582Z",
     "start_time": "2026-01-21T11:54:33.073188147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_second_tensor = torch.tensor([[4, 5, 6],[7, 8, 9]])\n",
    "my_second_tensor.device"
   ],
   "id": "15c96df50ae9897",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:33.102879905Z",
     "start_time": "2026-01-21T11:54:33.079416444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_second_tensor = my_second_tensor.to(\"cuda\")\n",
    "my_second_tensor.device"
   ],
   "id": "a6a73656409c991f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tensors need to be on the same device to allow operations between them.",
   "id": "c2e6a5c222007688"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:54:33.369359562Z",
     "start_time": "2026-01-21T11:54:33.300208123Z"
    }
   },
   "cell_type": "code",
   "source": "print(my_tensor + my_second_tensor) #this produces an error",
   "id": "d4786d8b93eba300",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmy_tensor\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmy_second_tensor\u001B[49m) \u001B[38;5;66;03m#this produces an error\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:57:34.681726736Z",
     "start_time": "2026-01-21T11:57:34.631553821Z"
    }
   },
   "cell_type": "code",
   "source": "print(my_tensor.to(\"cuda\") + my_second_tensor) #this does not",
   "id": "de5b8121caa263f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  7,  9],\n",
      "        [11, 13, 15]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "tags": []
   },
   "cell_type": "markdown",
   "source": [
    "#### Beware the views\n",
    "\n",
    "Just like in Numpy, some PyTorch operations return views and not hard copies. This is usually intuitive and intended, but can lead to accidents with in-place operations because different variable point to the same underlying data if not accounted for.\n",
    "The most important operations that return a view are:\n",
    "- (basic) indexing and slicing ops\n",
    "- Transposition (`.T`, `.transpose()`, ...)\n",
    "- `squeeze()`, `unsqueeze()`\n",
    "- some selection operations (`.select()`, `. diagonal()`, ...)\n",
    "\n",
    "It should also be noted that `.reshape()` can return either a view or a copy, meaning code should account for both possibilities.\n",
    "\n",
    "Also note that mathematical operations on views return copies, meaning they can be used safely without modifying the data underlying the view.\n",
    "\n",
    "The [Pytorch Documentation](https://docs.pytorch.org/docs/stable/tensor_view.html) contains a full list of all relevant operations."
   ],
   "id": "13467fa511af150c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:57:37.071259Z",
     "start_time": "2026-01-21T11:57:37.064107876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.tensor([[4, 5, 6],[7, 8, 9]])\n",
    "b = a.T"
   ],
   "id": "bd1c01276ee3418",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:57:38.075600352Z",
     "start_time": "2026-01-21T11:57:38.065183556Z"
    }
   },
   "cell_type": "code",
   "source": "b[0][0] = 100",
   "id": "52f4efd4ac0ebe6d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:57:38.951537626Z",
     "start_time": "2026-01-21T11:57:38.938444410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = a*2\n",
    "a"
   ],
   "id": "72d729ac3bb6d277",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[200,  10,  12],\n",
       "        [ 14,  16,  18]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:57:40.145048791Z",
     "start_time": "2026-01-21T11:57:40.131907153Z"
    }
   },
   "cell_type": "code",
   "source": "b",
   "id": "7caaf43305c17a54",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100,   7],\n",
       "        [  5,   8],\n",
       "        [  6,   9]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### You probably want to use nn.Module to create your models\n",
    "\n",
    "A good way to build your own neural network architectures is to use nn.Module. Just write out your own class that inherits from nn.Module and give it a forward method and you are good to go. The simplest way to build your own model is to chain various premade operations from `torch.nn` in a `nn.Sequential`. Tutorials for various architectures can be found all over the internet."
   ],
   "id": "dd1249eaeadc1126"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T11:57:42.466616257Z",
     "start_time": "2026-01-21T11:57:42.454532670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 5 * 5, 400),\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "1bf5a71df1d512f9",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A lot of PyTorch's functionality is build around this. Training a model that is defined like this is super straightforward. Just pass the model parameters to an optimizer, compute a loss, make the backward call and perform an optimization step.\n",
    "\n",
    "```\n",
    "loss = F.cross_entropy(x_hat, y) #compute some loss based on model outputs y_hat and labels y\n",
    "optimizer.zero_grad(set_to_none=True) #reseting the optimizer's gradients is good practice to avoid mixup between different training steps\n",
    "loss.backward() #compute gradients based on loss\n",
    "optimizer.step() #update model parameters\n",
    "```\n",
    "\n",
    "\n",
    "##### Iterating through Modules\n",
    "\n",
    "In some cases it might be necessary to build models as more complex datastructures. A classic case here is one big model with a set of smaller \"sub-models\" which are themselves implemented using `nn.Module` (e.g. attention heads in a transformer). If the sub-models can't reasonably each be stored as discrete parameters, storing them in a nn.Modulelist is generally best practice.\n"
   ],
   "id": "d62acc83e9528c57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Handling Datasets\n",
    "Small datsets (<100ish MB) can usually be stored in GPU memory just fine and accessed by iterating over them. Larger Datasets should be processed using a Dataloader (more info [here](https://docs.pytorch.org/docs/stable/data.html)).\n",
    "\n",
    "To make your dataloader more efficient set `numworkers > 1` (this enables asynchronous dataloading) and set `pin_memory=True` for faster transfers from system memory to GPU memory."
   ],
   "id": "eff8463c64f9930c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
