{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pytorch Crash Course",
   "id": "d4b1c432bf83343c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Quick Tips for installing Pytorch\n",
    "You need the CUDA Toolkit and an Nvidia GPU.\n",
    "\n",
    "Type\n",
    "```\n",
    "nvcc --version\n",
    "```\n",
    "into your console to figure out what CUDA version you have.\n",
    "\n",
    "If you don't get a return on your own computer, you might need to install the [CUDA Toolkit](https://developer.nvidia.com/cuda/toolkit).\n",
    "###### _If you are on a remote computer, you probably can't do that on your own. Talk to an admin instead._\n",
    "\n",
    "If everything is alright the return should look roughly like this:\n",
    "```\n",
    "nvcc: NVIDIA (R) Cuda compiler driver\n",
    "Copyright (c) 2005-2024 NVIDIA Corporation\n",
    "Built on Wed_Aug_14_10:26:51_Pacific_Daylight_Time_2024\n",
    "Cuda compilation tools, release 12.6, V12.6.68\n",
    "Build cuda_12.6.r12.6/compiler.34714021_0\n",
    "```\n",
    "(I have CUDA 12.6 on the machine I ran the command)\n",
    "\n",
    "Now you can go to [pytorch's official website](pytroch.org) to download pytorch for the correct CUDA version.\n",
    "\n",
    "Pytorch seems to be okay at dealing with CUDA version mismatches, I have seen newer versions of pytorch run fine with older CUDA versions. __DON'T QUOTE ME ON THIS THOUGH__\n",
    "\n",
    "Pytorch also offers [old versions](https://pytorch.org/get-started/previous-versions/) for download, which work with older CUDA versions.\n",
    "\n"
   ],
   "id": "9159a1dd2c6dca9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why Pytorch?\n",
    "\n",
    "- Differentiation is hard, PyTorch does it for you with one line of code\n",
    "- PyTorch implements a lot of other functionality as well (data handling, optimizers, model handling, loss funcitons, ...)\n",
    "- PyTorch runs fast"
   ],
   "id": "8435c3786c6b6ae7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why use GPUs for Machine Learning?\n",
    "\n",
    "#### A Demo:\n",
    "Let's fit a CNN to CIFAR10.\n",
    "(_This code is shamelessly copied from an exercise in Prof. Jain's FMI course from last year_)"
   ],
   "id": "4cdc649c457ebf50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T20:07:17.970848328Z",
     "start_time": "2026-01-19T20:07:15.430653608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.small_CNN import *\n",
    "\n",
    "def train(model, optimizer, device):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for batch_idx, (X, y) in enumerate(train_loader):\n",
    "            start_batch = time.time()\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            z = model(X)\n",
    "            loss = F.cross_entropy(z, y)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            end_batch = time.time()\n",
    "            batch_time = int((end_batch - start_batch) * 1000)\n",
    "        print(f\"epoch {epoch}, {batch_time}ms elapsed\")"
   ],
   "id": "78bac3aa4efb9261",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = Net(num_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "train(model, optimizer, device)"
   ],
   "id": "e9ac4e5267f1e9b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T20:07:45.768216295Z",
     "start_time": "2026-01-19T20:07:23.235359254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:2\")\n",
    "\n",
    "model = Net(num_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "train(model, optimizer, device)"
   ],
   "id": "d1cb456a09bac34c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, 4ms elapsed\n",
      "epoch 2, 2ms elapsed\n",
      "epoch 3, 2ms elapsed\n",
      "epoch 4, 2ms elapsed\n",
      "epoch 5, 1ms elapsed\n",
      "epoch 6, 1ms elapsed\n",
      "epoch 7, 1ms elapsed\n",
      "epoch 8, 2ms elapsed\n",
      "epoch 9, 1ms elapsed\n",
      "epoch 10, 2ms elapsed\n",
      "epoch 11, 1ms elapsed\n",
      "epoch 12, 2ms elapsed\n",
      "epoch 13, 3ms elapsed\n",
      "epoch 14, 2ms elapsed\n",
      "epoch 15, 2ms elapsed\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, running on the GPU can be a lot faster. But why is that the case?\n",
    "\n",
    "Long story short: Forward- and backward-passes (Inference and Training) require massive amounts of vector/matrix (tensor) operations, which are highly parallelizable. GPUs (Graphics Processing Units) are built to solve these kinds of highly parallel operations to render pretty computer graphics, so they are also good at neural networks by accident. Datacenter GPUs nowadays are often purpose built for ML tasks, so they are even better.\n",
    "\n",
    "If you do projects on your home computer and have a decent gaming GPU, then you can probably use it for ML.\n",
    "\n",
    "If you need big computing power, the KI-GPU Server at OTH Regensburg or the NHR at FAU Erlangen can give you access to big datacenter GPUs."
   ],
   "id": "80f2302998de3ebc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Okay but how do I PyTorch?\n",
    "\n",
    "#### Online Resources\n",
    "- [Official Documentation](https://docs.pytorch.org/docs/stable/index.html): For looking up syntax etc.\n",
    "- [Official Tutorials](https://docs.pytorch.org/tutorials/index.html): This is the best place to start; also teaches a lot of general ML things.\n",
    "- [Huggingface Tutorials](https://huggingface.co/learn): If you are specifically interested in NLP, then this will teach you just enough PyTorch to get by (while also teaching you a lot of other things).\n",
    "- [\"Let's build GPT: from scratch, in code, spelled out.\" by Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY): Not just for people interested in transformers! An excellent tutorial on working with PyTorch to build a complex model. If you are interested also look at [the follow up video on reproducing GPT-2](https://www.youtube.com/watch?v=l8pRSuU81PU)\n",
    "\n",
    "\n",
    "### You already know a lot of the syntax if you know Numpy...\n",
    "\n",
    "- PyTorch tensors represent n-dimensional arrays of various datatypes (just like ndarrays in numpy).\n",
    "- Doing linear Algebra with PyTorch is very similar to doing it with numpy.\n",
    "- A lot of numpy functions are named identically in PyTorch. (so just trying out what you would do in numpy often works)\n",
    "- Most of the ones that aren't identical have an equivalent you can find with a google search.\n",
    "\n"
   ],
   "id": "5700a9e270521bdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_tensor = torch.tensor([[1, 2, 3],[4, 5, 6]])\n",
    "my_tensor"
   ],
   "id": "145a7132926f14cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "my_tensor[1][1:]",
   "id": "875836e7d5baad89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "my_tensor.reshape((3,2))",
   "id": "1e2e3f041ffba89d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "my_tensor * 2",
   "id": "e4915927e1a95872",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "my_tensor.T",
   "id": "f9f8c9221997b375",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "my_tensor.unsqueeze(0)",
   "id": "1b599463ea886271",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "my_tensor @ my_tensor.T #matrix multiplication",
   "id": "cc6eec695a3afd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### The important new bit: The GPU\n",
    "Tensors need to explicitly be moved between devices."
   ],
   "id": "c2cc964aa7143204"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.cuda.is_available() #check if GPU is available",
   "id": "6e2ff99e29381227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_second_tensor = torch.tensor([[4, 5, 6],[7, 8, 9]])\n",
    "my_second_tensor.device"
   ],
   "id": "15c96df50ae9897",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_second_tensor = my_second_tensor.to(\"cuda\")\n",
    "my_second_tensor.device"
   ],
   "id": "a6a73656409c991f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tensors need to be on the same device to allow operations between them.",
   "id": "c2e6a5c222007688"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(my_tensor + my_second_tensor)",
   "id": "d4786d8b93eba300",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(my_tensor.to(\"cuda\") + my_second_tensor)",
   "id": "de5b8121caa263f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### You probably want to use nn.Module to create your models\n",
    "\n",
    "A good way to build your own neural network architectures is to use nn.Module. Just write out your own class that inherits from nn.MOdule and give it a forward method and you are good to go.\n",
    "\n",
    "TODO: more here?"
   ],
   "id": "dd1249eaeadc1126"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 5 * 5, 400),\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "1bf5a71df1d512f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Handling Datasets\n",
    "TODO\n",
    "- Having your data stored in memory can be a good or a bad idea\n",
    "- one tensor dimension is the batch, all data in a batch should be a tensor\n",
    "- process entire batch at once by feeding full tensor into your model\n",
    "- dataloader asynchronous stuff"
   ],
   "id": "eff8463c64f9930c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "18afd41e8e9fb436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_data_in_mem(model, optimizer, device, X, y):\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        for i in range(0, X.size(0), batch_size):\n",
    "            start_batch = time.time()\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            z = model(X)\n",
    "            loss = F.cross_entropy(z, y)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            end_batch = time.time()\n",
    "            batch_time = int((end_batch - start_batch) * 1000)\n",
    "        print(f\"epoch {epoch}, {batch_time}ms elapsed\")"
   ],
   "id": "f1e9377628ebd2d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:2\")\n",
    "\n",
    "train_data_raw = datasets.CIFAR10(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True\n",
    ")\n",
    "X = torch.tensor(train_data_raw.data, device=device, dtype=torch.float32)\n",
    "y = torch.tensor(train_data_raw.targets, device=device)\n",
    "norm = torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "X = norm(torch.transpose(X,-1,-3))\n",
    "\n",
    "\n",
    "model = Net(num_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "train_data_in_mem(model, optimizer, device, X, y)"
   ],
   "id": "dc35a2cd4fb7c338",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "nn.Modulelist if you need to iterate through modules!!!! this is particularly import for compile (see below)\n",
    "\n",
    "### The CPU-GPU Bottleneck"
   ],
   "id": "c9677f2d141ee662"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda:2\")\n",
    "\n",
    "X,y = get_data(device)\n",
    "\n",
    "model = Net(num_classes)\n",
    "model = model.to(device)\n",
    "model = torch.compile(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    for i in range(0, X.size(0), batch_size):\n",
    "        z = model(X[i:i + batch_size])\n",
    "        a = int(X[0][0][0][0])\n",
    "        loss = F.cross_entropy(z, y[i:i + batch_size])\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "id": "e6a03315fa394475"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Views and Copies\n",
    "\n",
    "this needed?"
   ],
   "id": "df732950436741d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
